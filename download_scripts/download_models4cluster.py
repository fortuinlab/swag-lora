from transformers import (
    AutoModelForSequenceClassification, 
    AutoModelForMultipleChoice, 
    AutoTokenizer, 
    AutoModelForCausalLM
)
import os
import torch

def download_model(model_name, save_directory):
    if not os.path.exists(save_directory):
        os.makedirs(save_directory)

    # Downloading the model and tokenizer
    cache_dir = os.path.join(save_directory, model_name)
    if model_name in ['roberta-base', 'roberta-large']:
        AutoModelForSequenceClassification.from_pretrained(model_name).save_pretrained(cache_dir)
        AutoModelForMultipleChoice.from_pretrained(model_name).save_pretrained(cache_dir)
        AutoTokenizer.from_pretrained(model_name).save_pretrained(cache_dir)
    
    elif model_name == 'meta-llama/Llama-2-7b-hf':
        device = "cuda"
        dtype = torch.bfloat16
     
        trash_dir = '/p/scratch/hai_baylora/trash'
        AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=dtype, cache_dir=trash_dir).save_pretrained(cache_dir)
        AutoTokenizer.from_pretrained(model_name).save_pretrained(cache_dir)
    
    elif model_name == 'meta-llama/Llama-2-7b-chat-hf':
        device = "cuda"
        dtype = torch.bfloat16
   
        AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=dtype).save_pretrained(cache_dir)
        AutoTokenizer.from_pretrained(model_name).save_pretrained(cache_dir)
    
    else:
        # Add other model download steps if neededâ€³
        pass

    print(f"Model {model_name} downloaded and saved to {save_directory}")

# Example usage
#download_model('roberta-base', '/p/scratch/hai_baylora/models')
#download_model('roberta-large', '/p/scratch/hai_baylora/models')
download_model('meta-llama/Llama-2-7b-hf', '/p/scratch/hai_baylora/models2')
